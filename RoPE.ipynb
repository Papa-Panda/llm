{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIY07SA1FjFHwE/qrFTYaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/llm/blob/main/RoPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gemini.google.com/app/4e18d355af4f8a38\n",
        "\n",
        "# 【DeepSeek-v2 MLA 原理讲解】 https://www.bilibili.com/video/BV1BYXRYWEMj/?share_source=copy_web&vd_source=985107e9bc8449878c67f709b64e7ad2"
      ],
      "metadata": {
        "id": "TvJuplBuRrWV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpHna5myNwq7",
        "outputId": "d5767446-2487-4077-88cc-a82146a7cc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input shape: torch.Size([2, 10, 64])\n",
            "Original input (first element):\n",
            "tensor([ 1.8229, -1.1492,  0.4515,  1.8060])\n",
            "\n",
            "Output shape with RoPE: torch.Size([2, 10, 64])\n",
            "Output with RoPE (first element):\n",
            "tensor([ 1.8229, -1.1492,  0.4515,  1.8060])\n",
            "\n",
            "Demonstrating relative positioning effects:\n",
            "Dot product of original token 0 and token 5: -11.9919\n",
            "Dot product of RoPE-applied token 0 and token 5: -8.6927\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class RotaryPositionEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048, theta=10000.0):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.theta = theta\n",
        "\n",
        "        # Precompute the inverse frequencies\n",
        "        # These are used to calculate the rotation angles for each dimension\n",
        "        # and each position.\n",
        "        inv_freq = 1.0 / (self.theta ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        # Precompute the positions (m)\n",
        "        # We'll use these to generate the angles for each position.\n",
        "        self.cached_angles = None\n",
        "        self.cached_seq_len = None\n",
        "\n",
        "    def _rotate_half(self, x):\n",
        "        # Rotates the input tensor by half its dimension.\n",
        "        # This is a core operation in RoPE, enabling the complex-number-like rotation.\n",
        "        x1, x2 = x[..., : self.dim // 2], x[..., self.dim // 2 :]\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.shape[1] # Assuming x has shape (batch_size, seq_len, dim)\n",
        "\n",
        "        # If the sequence length changes, or angles haven't been computed yet, recompute them.\n",
        "        if self.cached_seq_len is None or seq_len > self.cached_seq_len:\n",
        "            self.cached_seq_len = seq_len\n",
        "            # Generate the positions (m) for the current sequence length.\n",
        "            seq_idx = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            # Calculate the angles for each position and each frequency.\n",
        "            # Shape: (seq_len, dim // 2)\n",
        "            angles = torch.einsum(\"i,j->ij\", seq_idx, self.inv_freq)\n",
        "\n",
        "            # Expand angles to cover the full dimension, duplicating for sin and cos parts.\n",
        "            # Shape: (seq_len, dim)\n",
        "            self.cached_angles = torch.cat((angles, angles), dim=-1)\n",
        "\n",
        "        # Get the angles for the current sequence length\n",
        "        angles = self.cached_angles[:seq_len, :].unsqueeze(0) # Add batch dimension\n",
        "\n",
        "        # Apply the rotation\n",
        "        # cos(angles) * x + sin(angles) * _rotate_half(x)\n",
        "        # This is equivalent to multiplying by e^(i * angles) in the complex plane.\n",
        "        x_rotated = x * angles.cos() + self._rotate_half(x) * angles.sin()\n",
        "        return x_rotated\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define parameters\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "    model_dim = 64  # Must be an even number for this implementation of RoPE\n",
        "\n",
        "    # Create a dummy input tensor (e.g., embeddings)\n",
        "    # Shape: (batch_size, seq_len, model_dim)\n",
        "    dummy_input = torch.randn(batch_size, seq_len, model_dim)\n",
        "\n",
        "    print(f\"Original input shape: {dummy_input.shape}\")\n",
        "    print(f\"Original input (first element):\\n{dummy_input[0, 0, :4]}\")\n",
        "\n",
        "    # Initialize RoPE\n",
        "    rope = RotaryPositionEmbedding(dim=model_dim)\n",
        "\n",
        "    # Apply RoPE to the input\n",
        "    output_with_rope = rope(dummy_input)\n",
        "\n",
        "    print(f\"\\nOutput shape with RoPE: {output_with_rope.shape}\")\n",
        "    print(f\"Output with RoPE (first element):\\n{output_with_rope[0, 0, :4]}\")\n",
        "\n",
        "    # Demonstrate relative positioning\n",
        "    # The dot product between two tokens at a certain relative distance\n",
        "    # should be related to the angle between their rotated embeddings.\n",
        "\n",
        "    # Let's consider two tokens from the same sequence\n",
        "    token_0 = dummy_input[0, 0]\n",
        "    token_5 = dummy_input[0, 5]\n",
        "\n",
        "    token_0_rope = output_with_rope[0, 0]\n",
        "    token_5_rope = output_with_rope[0, 5]\n",
        "\n",
        "    print(\"\\nDemonstrating relative positioning effects:\")\n",
        "    print(f\"Dot product of original token 0 and token 5: {torch.dot(token_0, token_5):.4f}\")\n",
        "    print(f\"Dot product of RoPE-applied token 0 and token 5: {torch.dot(token_0_rope, token_5_rope):.4f}\")\n",
        "\n",
        "    # The exact relationship is that the dot product of two RoPE-applied vectors\n",
        "    # depends on the original dot product and the cosine of the angle corresponding\n",
        "    # to their relative position.\n",
        "    # Mathematically, for q_m and k_n (query at position m, key at position n):\n",
        "    # RoPE(q_m) . RoPE(k_n) = q_m . k_n * cos(theta_m - theta_n) + ... (more complex terms)\n",
        "    # The key idea is that the relative position (m-n) is encoded into the rotation.\n",
        "    # This leads to the desirable property that attention scores are functions of relative distance."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLwTh5peRqNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}