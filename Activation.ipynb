{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl6mmSuGQOwpSTzCV2Ot2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/llm/blob/main/Activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0LqAOnBLaUk"
      },
      "outputs": [],
      "source": [
        "# https://gemini.google.com/app/90bd61d394533684\n",
        "\n",
        "# 为什么现在的大模型要高精度跑GeLU或SwiGLU，而不是改回ReLU跑低精度？ - 伊斯特伍德的回答 - 知乎\n",
        "# https://www.zhihu.com/question/15527003900/answer/129423629478"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, input_dim: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        # This linear layer effectively creates two sets of outputs:\n",
        "        # one for the main branch and one for the gating branch.\n",
        "        self.linear = nn.Linear(input_dim, 2 * output_dim)\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Step 1: Apply the combined linear transformation\n",
        "        # The output tensor 'gate_and_main' will have features for both\n",
        "        # the gating and the main branches.\n",
        "        # Shape: (batch_size, sequence_length, 2 * output_dim)\n",
        "        gate_and_main = self.linear(x)\n",
        "\n",
        "        # Step 2: Split the tensor into two halves along the last dimension\n",
        "        # (batch_size, sequence_length, output_dim) for each part\n",
        "        gate, main = torch.split(gate_and_main, self.output_dim, dim=-1)\n",
        "\n",
        "        # Step 3: Apply Swish (SiLU) activation to the gate branch\n",
        "        swished_gate = F.silu(gate) # F.silu is PyTorch's Swish/SiLU\n",
        "\n",
        "        # Step 4: Element-wise multiply the swished gate with the main branch\n",
        "        output = swished_gate * main\n",
        "\n",
        "        return output\n",
        "\n",
        "# --- Example Usage (PyTorch) ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Define input dimensions\n",
        "    input_dim = 768  # Common embedding dimension in LLMs\n",
        "    output_dim = 2048 # Common hidden dimension size\n",
        "\n",
        "    # Create an instance of the SwiGLU layer\n",
        "    swiglu_layer = SwiGLU(input_dim, output_dim)\n",
        "    print(f\"PyTorch SwiGLU layer: {swiglu_layer}\")\n",
        "\n",
        "    # Create a dummy input tensor\n",
        "    # batch_size = 2, sequence_length = 10, input_dim = 768\n",
        "    dummy_input = torch.randn(2, 10, input_dim)\n",
        "    print(f\"PyTorch Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "    # Pass the input through the SwiGLU layer\n",
        "    output = swiglu_layer(dummy_input)\n",
        "    print(f\"PyTorch Output shape: {output.shape}\")\n",
        "\n",
        "    # Verify output dimensions\n",
        "    assert output.shape == (2, 10, output_dim), \"PyTorch Output shape mismatch!\"\n",
        "    print(\"PyTorch SwiGLU implementation test passed!\")\n",
        "\n",
        "    # --- Another common pattern in LLMs: applying a final linear layer after SwiGLU ---\n",
        "    # This is often done in the feed-forward network (FFN) block.\n",
        "    # The output of SwiGLU (e.g., 2048 features) is then projected back\n",
        "    # to the original embedding dimension (e.g., 768 features).\n",
        "    final_projection_layer = nn.Linear(output_dim, input_dim)\n",
        "    final_output = final_projection_layer(output)\n",
        "    print(f\"PyTorch Final output after projection shape: {final_output.shape}\")\n",
        "    assert final_output.shape == (2, 10, input_dim), \"PyTorch Final output shape mismatch!\"\n",
        "    print(\"PyTorch Full FFN-like block (SwiGLU + projection) test passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5b3gYzkLgaE",
        "outputId": "ca36f59b-6c7b-4b2f-bb53-d0bee4a31c8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch SwiGLU layer: SwiGLU(\n",
            "  (linear): Linear(in_features=768, out_features=4096, bias=True)\n",
            ")\n",
            "PyTorch Dummy input shape: torch.Size([2, 10, 768])\n",
            "PyTorch Output shape: torch.Size([2, 10, 2048])\n",
            "PyTorch SwiGLU implementation test passed!\n",
            "PyTorch Final output after projection shape: torch.Size([2, 10, 768])\n",
            "PyTorch Full FFN-like block (SwiGLU + projection) test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZtWEi8qwLm_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}