{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHkVLjGhMq3SplMF0mEj7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/llm/blob/main/RLHF_instruct_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supervised Fine-Tuning (SFT): Train a base language model on labeled datasets to perform specific instructions."
      ],
      "metadata": {
        "id": "8TOiUNOhyj_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL: https://wandb.ai/settings#api\n",
        "# # api_key: 762475e7d3a86eb2d9ad7cf3860dcbafb1a1448a\n",
        "# !pip install wandb -qU\n",
        "# import wandb\n",
        "# wandb.login()"
      ],
      "metadata": {
        "id": "ObapDN1V1s2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "c7919efbc6e0463198406dce9280dfee",
            "38a4c9d174804d1e837d9e7f1ae3808f",
            "fad65d8db4c44da2bbafb5b99f90f6c6",
            "a1cf906056244ec1b5562f7596d60853",
            "0cac8a69d92443e9bdcb41f62d94ba74",
            "2d8ef9a2d20a482ab5c2fc108dbc8ba0",
            "eda5bd45107c439b9a6220b71e46341b",
            "83a5751ef1a94ad0ba2520a8ac26089d",
            "282f8574c01a475b811c981ae1065441",
            "ff09ef728b9a4edaa349c15e1dd6ee57",
            "2c25a5462f2a409bbb5626bb7807ac98",
            "b28ea7ebe05c4dfb8909fcb5eb2f7ab0",
            "6b4695d5a8414888ae6d144d72291096",
            "fefc55b97fc342d09586c0d007b7ef3f",
            "a737fb9318ed4784b0b4d6cd3e5b5c1c",
            "7ca3fa3ac862422db7922c0ecc5e7ffa",
            "2b3e06d0bb6c4515bd401945b35c547a",
            "402d1fc131c44ef9bd391571d6387edc",
            "fd48eca7ab914beb8ad9ab633890b1d2",
            "5019994a85984e2fb2b2b110799a3648",
            "e2ed25b02376418298cd5d60dc880015",
            "8648dfb91fcd4bcbbc651c084a5a3fec",
            "9d8ff461a84140d6bcee7ce071137902",
            "a57a5a3b54ec4ca4abbe7a43e0f4183e",
            "765d28d1228f42229d7cf126f537d56d",
            "2b84778d5bf448cd9f812289344f19c9",
            "3ef4e7e613804fe4a2bdcb0c56a3e570",
            "72eb6af59b3048fcb0d7e0f78a29b9bc",
            "5e87a3f42da74a0ba51ac67c39fc920b",
            "9d017175e0284f828f9ced8d712630bb",
            "791ad54bb3be4bf8b35aac246a7eab3c",
            "ca35846ca1894d448a7190029ac0df65",
            "16ebfc07575a4d358615ca11e0456fb3",
            "1338897ab5e6428ca6c40eb303e5e481",
            "7728d7032c1a4ad8bac0f6159c3b3f7b",
            "ff13c65a11dc4a489823d6ca6d94951b",
            "3251574a74d54acf8a5a911db0706cdb",
            "4a64bcd1365c42659fc4f56643aac7ec",
            "93dac9297a2447768024280499bc902c",
            "4b6abdd817bc45d1959d2ce6201d3355",
            "88037af974f147a4a56682579a61ab5f",
            "64ae5aa1ea3540fb9ed509ba8e97646e",
            "01c30652fc0a4238831ff2f60c894b7b",
            "c707413612d64e23a3d5d809da23864b",
            "5fdca58da0934879868d29688cb7bfa4",
            "043c3663ba644319ab986bc475df418d",
            "dbe0d587fba143b3bfce0f6cd4868334",
            "92e14035b49b4d9cb4e6b10b3272756e",
            "9f81d284f75f431693c702b0f19143ae",
            "8299307687464067bd2d7e35d74d0339",
            "30dbfd0d56b447989358751f37eeb152",
            "92d4042332e1478d80cfac2d3df9c42f",
            "53dcfd589c3b4a3887d554799d284510",
            "d4a30a18624c41a19987439946ec49ab",
            "4ed1e16fb4164cbdb2b839aeca3c9a24",
            "1d0073e23acc44879c5b1b4853c408c3",
            "6439859a46c74d2783aa28352caaa3b1",
            "efd93951f50f42f899266baec6df6782",
            "d70817d0794a432db88b8560c5a38f06",
            "c5014414a3ff48b4918bdab084cfbd38",
            "c410049739224f6cbb0c2c105b2e6c07",
            "62d9e46bbfc448dca42a93a7c828d9d3",
            "fdd7c155e37442f984776af28bd51b8c",
            "08206a633d734ee28317fc737c728cba",
            "1a1c139cf1824f119e589f98d143bd25",
            "7e70519de28041ff82f80483efb35fb6",
            "e3e801e6f8cf47a7a3395bd71635bd58",
            "53693d1d57434d2d915d4fe7a2eb8d78",
            "5cef4db9120c4982b6adefb7a1c85d2e",
            "18e171f2c4dd4af786edac05b2b1ffc5",
            "69f192b85bf448298d464c6acc419800",
            "c7f4d90217304dc1a7f2df7e7935ad81",
            "788ffde8f95346318392d1dfbd18fec1",
            "19bb03a498c24b09b1672bcf1ecdcf48",
            "9946fec41551414caccbb48479fb07d2",
            "0ba73e1f9c9d46778d2b2921392bd190",
            "2cb6ab30955c41dc86a270ca91b10587"
          ]
        },
        "id": "uivRZ52gyNt_",
        "outputId": "99b170e5-0d71-4fdd-c71b-61c263640d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7919efbc6e0463198406dce9280dfee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b28ea7ebe05c4dfb8909fcb5eb2f7ab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d8ff461a84140d6bcee7ce071137902"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1338897ab5e6428ca6c40eb303e5e481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fdca58da0934879868d29688cb7bfa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d0073e23acc44879c5b1b4853c408c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3e801e6f8cf47a7a3395bd71635bd58"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Mock dataset: Instruction-Output pairs\n",
        "dataset = [\n",
        "    {\"instruction\": \"Translate 'Hello' to Spanish.\", \"output\": \"Hola\"},\n",
        "    {\"instruction\": \"What is 2 + 2?\", \"output\": \"4\"},\n",
        "    {\"instruction\": \"Write a short poem about the moon.\", \"output\": \"The moon glows bright, in the silent night.\"}\n",
        "]\n",
        "\n",
        "\n",
        "# Preprocess dataset\n",
        "# Set the pad_token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def preprocess(data):\n",
        "    inputs = [f\"Instruction: {d['instruction']}\\nOutput:\" for d in data]\n",
        "    labels = [f\"{d['output']}\" for d in data]\n",
        "    tokenized = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    tokenized_labels = tokenizer(labels, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n",
        "    tokenized['labels'] = tokenized_labels\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = preprocess(dataset)\n",
        "\n",
        "# Fine-tuning settings\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=2,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_steps=10,\n",
        "#     save_steps=1000,\n",
        "#     save_total_limit=1\n",
        "# )\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=100,  # Save checkpoints every 100 steps\n",
        "    save_total_limit=1,  # Keep only the latest checkpoint\n",
        "    save_strategy=\"epoch\"  # Save the model at the end of each epoch\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "# trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()\n",
        "\n",
        "# Save the model and tokenizer manually\n",
        "trainer.save_model(\"./results\")  # Save the fine-tuned model\n",
        "tokenizer.save_pretrained(\"./results\")  # Save the tokenizer\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ1ViB8qasl5",
        "outputId": "e0148bdc-922a-4692-bf30-2c07e9b2bc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./results/tokenizer_config.json',\n",
              " './results/special_tokens_map.json',\n",
              " './results/vocab.json',\n",
              " './results/merges.txt',\n",
              " './results/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./results\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./results\")\n"
      ],
      "metadata": {
        "id": "1bZOBy9ma8C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward Model Training (RMT): Use human-labeled rankings of model outputs to train a reward model that scores outputs based on their alignment with instructions."
      ],
      "metadata": {
        "id": "IaYDDbgYyS6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Mock reward data: Ranked outputs\n",
        "# reward_data = [\n",
        "#     {\"instruction\": \"Translate 'Hello' to Spanish.\", \"outputs\": [\"Hola\", \"Ola\", \"Hi\"], \"ranking\": [2, 1, 0]},\n",
        "#     # {\"instruction\": \"What is 2 + 2?\", \"outputs\": [\"4\", \"four\", \"5\"], \"ranking\": [2, 1, 0]},\n",
        "# ]\n",
        "\n",
        "# # # Mock Reward Model: Simple ranking system\n",
        "# # def reward_model(output, outputs, ranking):\n",
        "# #     if output in outputs:\n",
        "# #         index = outputs.index(output)\n",
        "# #         return ranking[index]\n",
        "# #     else:\n",
        "# #         raise ValueError(f\"Output '{output}' not found in outputs list.\")\n",
        "# class RewardModel(nn.Module):\n",
        "#     def __init__(self, base_model_name=\"gpt2\"):\n",
        "#         super(RewardModel, self).__init__()\n",
        "#         # Load pre-trained language model\n",
        "#         self.base_model = AutoModel.from_pretrained(base_model_name)\n",
        "#         # Add a linear layer for scalar reward prediction\n",
        "#         self.reward_head = nn.Linear(self.base_model.config.hidden_size, 1)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask=None):\n",
        "#         # Extract hidden states from the base model\n",
        "#         outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         hidden_states = outputs.last_hidden_state\n",
        "#         # Use the [CLS] token hidden state (or first token for GPT)\n",
        "#         cls_embedding = hidden_states[:, 0, :]\n",
        "#         # Predict reward\n",
        "#         reward = self.reward_head(cls_embedding)\n",
        "#         return reward\n",
        "\n",
        "\n",
        "\n",
        "# # Convert outputs to numeric IDs\n",
        "# reward_dataset = []\n",
        "# for item in reward_data:\n",
        "#     for i, output in enumerate(item[\"outputs\"]):\n",
        "#         reward_dataset.append((item[\"instruction\"], output, item[\"ranking\"][i]))\n",
        "\n",
        "# # Example for demonstration:\n",
        "# # In practice, this would involve training a neural network on the reward dataset.\n"
      ],
      "metadata": {
        "id": "XnyY2Yy6yypK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Reward Model Definition\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, base_model_name=\"gpt2\"):\n",
        "        super(RewardModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
        "        self.reward_head = nn.Linear(self.base_model.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Use the first token\n",
        "        reward = self.reward_head(cls_embedding)\n",
        "        return reward\n",
        "\n",
        "# Mock reward data: Instructions, outputs, and rankings\n",
        "reward_data = [\n",
        "    {\"instruction\": \"Translate 'Hello' to Spanish.\", \"outputs\": [\"Hola\", \"Ola\", \"Hi\"], \"ranking\": [2, 1, 0]},\n",
        "    {\"instruction\": \"What is 2 + 2?\", \"outputs\": [\"4\", \"five\", \"3\"], \"ranking\": [2, 0, 0]},\n",
        "]\n",
        "\n",
        "# Prepare reward data\n",
        "def prepare_reward_data(data, tokenizer):\n",
        "    inputs, rewards = [], []\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    for item in data:\n",
        "        for output, rank in zip(item[\"outputs\"], item[\"ranking\"]):\n",
        "            input_text = f\"Instruction: {item['instruction']}\\nOutput: {output}\"\n",
        "\n",
        "            tokenized = tokenizer(input_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "            inputs.append(tokenized)\n",
        "            rewards.append(rank)\n",
        "    return inputs, torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "reward_model = RewardModel(base_model_name=\"gpt2\")\n",
        "reward_model.train()\n",
        "\n",
        "inputs, labels = prepare_reward_data(reward_data, tokenizer)\n",
        "\n",
        "# Train reward model\n",
        "optimizer = optim.Adam(reward_model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.MSELoss()\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, input_data in enumerate(inputs):\n",
        "        input_ids = input_data[\"input_ids\"]\n",
        "        attention_mask = input_data[\"attention_mask\"]\n",
        "        optimizer.zero_grad()\n",
        "        predictions = reward_model(input_ids, attention_mask)\n",
        "        loss = loss_fn(predictions, labels[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4reatmujniM0",
        "outputId": "0036741f-5b14-48c1-b034-74404a8ff77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.8012\n",
            "Epoch 2, Loss: 11.1984\n",
            "Epoch 3, Loss: 6.1715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning (RL): Fine-tune the model using reinforcement learning to optimize the reward model's score."
      ],
      "metadata": {
        "id": "DXrImNylyw6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# from torch.optim import Adam\n",
        "\n",
        "# # Load the fine-tuned model\n",
        "# fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./results\")\n",
        "\n",
        "# # PPO setup\n",
        "# optimizer = Adam(fine_tuned_model.parameters(), lr=5e-5)\n",
        "\n",
        "# # Mock PPO Loop\n",
        "# for epoch in range(5):  # Simulate 5 epochs of RL\n",
        "#     for item in reward_data:\n",
        "#         instruction = item[\"instruction\"]\n",
        "#         outputs = item[\"outputs\"]\n",
        "#         ranking = item[\"ranking\"]\n",
        "\n",
        "#         # Generate new output and compute reward\n",
        "#         response = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)(f\"Instruction: {instruction}\\nOutput:\")\n",
        "#         generated_text = response[0][\"generated_text\"]\n",
        "#         print(generated_text)\n",
        "#         # reward = reward_model(generated_text, outputs, ranking)\n",
        "#         reward = reward_model('Hola', outputs, ranking)\n",
        "\n",
        "#         # Optimize using the reward\n",
        "#         loss = -reward  # Negative reward as loss\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ],
      "metadata": {
        "id": "dN3al1gCyztC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "# policy_model = GPT2LMHeadModel.from_pretrained(\"./sft_model\")\n",
        "# policy_tokenizer = GPT2Tokenizer.from_pretrained(\"./sft_model\")\n",
        "# results\n",
        "policy_model = GPT2LMHeadModel.from_pretrained(\"./results\")\n",
        "policy_tokenizer = GPT2Tokenizer.from_pretrained(\"./results\")\n",
        "\n",
        "# Reward Pipeline\n",
        "reward_model.eval()\n",
        "\n",
        "# Simplified RLHF loop\n",
        "optimizer = optim.Adam(policy_model.parameters(), lr=5e-5)\n",
        "\n",
        "for step in range(10):  # 10 RL steps\n",
        "    # Mock instruction\n",
        "    instruction = \"Translate 'Hello' to Spanish.\"\n",
        "    input_prompt = f\"Instruction: {instruction}\\nOutput:\"\n",
        "\n",
        "    # Generate response\n",
        "    response = pipeline(\"text-generation\", model=policy_model, tokenizer=policy_tokenizer)(\n",
        "        input_prompt, max_length=20, num_return_sequences=1, return_full_text=False\n",
        "    )\n",
        "    generated_output = response[0][\"generated_text\"]\n",
        "\n",
        "    # Compute reward\n",
        "    tokenized_input = tokenizer(f\"{input_prompt}{generated_output}\", return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        reward = reward_model(tokenized_input[\"input_ids\"]).item()\n",
        "\n",
        "    # Reinforcement learning: Reward maximization\n",
        "    loss = -torch.tensor(reward)  # Minimize negative reward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Step {step + 1}, Reward: {reward:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "M97-AGyjy4wa",
        "outputId": "08a5fe5d-a41d-4d27-d824-e46e176b6c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b93cc6864a84>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Minimize negative reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mK_5S5aEbHYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yY2Zbf_ubhZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
